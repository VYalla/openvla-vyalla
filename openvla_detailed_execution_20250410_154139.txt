2025-04-10 15:41:39,093 - INFO - === OpenVLA Local Inference Started ===
2025-04-10 15:41:39,093 - INFO - Arguments: {'image_path': '/home/vishal/Documents/openvla_testv1/LIBERO/vyalla_scripts/all_demos/ketchup/episode_demo_44/frame_0002.png', 'instruction': 'pick up the ketchup bottle', 'checkpoint_path': '/home/vishal/Documents/openvla_testv1/openvla-vyalla/checkpoints/openvla-7b-prismatic/checkpoints/step-295000-epoch-40-loss=0.2200.pt', 'device': 'cuda:0', 'log_file': 'openvla_detailed_execution_20250410_154139.txt'}
2025-04-10 15:41:39,093 - INFO - Importing model components...
2025-04-10 15:41:40,058 - DEBUG - Falling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.
2025-04-10 15:41:40,095 - DEBUG - Creating converter from 7 to 5
2025-04-10 15:41:40,095 - DEBUG - Creating converter from 5 to 7
2025-04-10 15:41:40,095 - DEBUG - Creating converter from 7 to 5
2025-04-10 15:41:40,095 - DEBUG - Creating converter from 5 to 7
PRINT: Loading local OpenVLA model from /home/vishal/Documents/openvla_testv1/openvla-vyalla/checkpoints/openvla-7b-prismatic/checkpoints/step-295000-epoch-40-loss=0.2200.pt...


===== STARTING MODEL LOADING =====



===== MODEL STRUCTURE =====

Model type: OpenVLA
Model device: cpu
Vision backbone: DinoSigLIPViTBackbone
LLM backbone: LLaMa2LLMBackbone
Action tokenizer: ActionTokenizer
Normalization stats keys: ['fractal20220817_data', 'kuka', 'bridge_orig', 'taco_play', 'jaco_play', 'berkeley_cable_routing', 'roboturk', 'viola', 'berkeley_autolab_ur5', 'toto', 'stanford_hydra_dataset_converted_externally_to_rlds', 'austin_buds_dataset_converted_externally_to_rlds', 'nyu_franka_play_dataset_converted_externally_to_rlds', 'furniture_bench_dataset_converted_externally_to_rlds', 'ucsd_kitchen_dataset_converted_externally_to_rlds', 'austin_sailor_dataset_converted_externally_to_rlds', 'austin_sirius_dataset_converted_externally_to_rlds', 'dlr_edan_shared_control_converted_externally_to_rlds', 'iamlab_cmu_pickup_insert_converted_externally_to_rlds', 'utaustin_mutex', 'berkeley_fanuc_manipulation', 'cmu_stretch', 'bc_z', 'fmb_dataset', 'dobbe']

==============================================================
                DETAILED EXECUTION FLOW                  
==============================================================

[1] Starting inference with instruction: 'pick up the ketchup bottle'
PRINT: 
=== DETAILED EXECUTION FLOW ===
PRINT: [1] Starting inference with instruction: 'pick up the ketchup bottle'
PRINT: [2] Attempting to use model.predict_action() method
PRINT:     - Model type: OpenVLA
PRINT:     - Model device: cpu
PRINT:     - Vision backbone: DinoSigLIPViTBackbone
PRINT:     - LLM backbone: LLaMa2LLMBackbone
PRINT: [3] Encountered error: 'OpenVLA' object has no attribute '_supports_cache_class'
PRINT: [4] Switching to alternative approach (manual pipeline)
PRINT: [5] Getting image transform and tokenizer components
PRINT:     - Image transform type: DinoSigLIPImageTransform
PRINT:     - Tokenizer type: LlamaTokenizerFast
PRINT: [6] Building prompt: 'In: What action should the robot take to pick up the ketchup bottle?
Out: '
PRINT: [7] Tokenizing prompt text
PRINT:     - Tokenizer output type: BatchEncoding
PRINT:     - Tokenizer output keys: ['input_ids', 'attention_mask']
PRINT: [8] Moving input_ids to device: cpu
PRINT:     - Input IDs shape: torch.Size([1, 23])
PRINT:     - Token sequence length: 23
PRINT: [9] Handling special tokens for Llama model (eos_token_id=2)
PRINT: [11] Processing image with vision backbone
PRINT:     - Processed image type: dict
PRINT:     - Pixel values keys: ['dino', 'siglip']
PRINT: [12] Adding batch dimension to each value in pixel values dict
PRINT:     - dino shape: torch.Size([1, 3, 224, 224])
PRINT:     - siglip shape: torch.Size([1, 3, 224, 224])
PRINT: [13] Running model forward pass (instead of generate)
PRINT:     - Input IDs shape: torch.Size([1, 23])
PRINT:     - Pixel values type: dict
PRINT: [14] Starting torch.no_grad() context for inference
PRINT:     - Calling model.__call__ with input_ids and pixel_values
PRINT:     - Forward pass complete
PRINT: [15] Model outputs type: CausalLMOutputWithPast
PRINT:     - Output logits shape: torch.Size([1, 279, 32064])
PRINT: [16] Extracting token IDs and decoding to actions
PRINT:     - Action dimension: 7
PRINT: [17] Taking argmax over vocabulary dimension to get predicted token IDs
PRINT:     - Predicted token IDs shape: torch.Size([7])
PRINT:     - Token IDs: [31878, 31871, 31875, 31779, 31862, 31881, 31874]
PRINT: [18] Decoding token IDs to normalized actions using ActionTokenizer
PRINT:     - Normalized actions shape: (7,)
PRINT:     - Normalized actions: [-0.04705882  0.00784314 -0.02352941  0.72941176  0.07843137 -0.07058824
 -0.01568627]
PRINT: [19] Un-normalizing actions to real-world values
PRINT:     - Action stats keys: ['mean', 'std', 'max', 'min', 'q01', 'q99', 'mask']
PRINT:     - Action mask type: list
PRINT:     - Action mask length: 7
PRINT:     - Action high (q99): [0.02830968 0.04085525 0.04016159 0.08192048 0.07792851 0.20382574
 1.        ]
PRINT:     - Action low (q01): [-0.02872725 -0.0417035  -0.02609386 -0.08092105 -0.092887   -0.20718276
  0.        ]
PRINT: [20] Applying un-normalization formula: 0.5 * (norm_actions + 1) * (high - low) + low
PRINT:     - Final action shape: (7,)
PRINT:     - Final action values: [-0.00155083 -0.00010036  0.00625439  0.05988898 -0.0007806  -0.01618469
 -0.01568627]
PRINT: [21] Action prediction pipeline complete
PRINT: 
Predicted Action (7-DoF):
PRINT: [-0.00155083 -0.00010036  0.00625439  0.05988898 -0.0007806  -0.01618469
 -0.01568627]
PRINT: 
Action components:
PRINT:   x: -0.0016
PRINT:   y: -0.0001
PRINT:   z: 0.0063
PRINT:   roll: 0.0599
PRINT:   pitch: -0.0008
PRINT:   yaw: -0.0162
PRINT:   gripper: -0.0157

==============================================================
                   EXECUTION SUMMARY                     
==============================================================

Model: OpenVLA
Vision backbone: DinoSigLIPViTBackbone
LLM backbone: LLaMa2LLMBackbone
Input image: /home/vishal/Documents/openvla_testv1/LIBERO/vyalla_scripts/all_demos/ketchup/episode_demo_44/frame_0002.png
Instruction: 'pick up the ketchup bottle'
Output action: [-0.00155083 -0.00010036  0.00625439  0.05988898 -0.0007806  -0.01618469
 -0.01568627]
PRINT: 
Detailed execution log saved to: openvla_detailed_execution_20250410_154139.txt
